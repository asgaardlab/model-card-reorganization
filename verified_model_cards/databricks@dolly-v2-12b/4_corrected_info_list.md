## Summary
- Databricks' `dolly-v2-12b` is an instruction-following large language model.
- It is trained on the Databricks machine learning platform.
- The model is licensed for commercial use.
- `dolly-v2-12b` is based on `pythia-12b`.
- It is trained on approximately 15,000 instruction/response fine-tuning records.
- The instruction/response records are from [`databricks-dolly-15k`](https://github.com/databrickslabs/dolly/tree/master/data).
- The records were generated by Databricks employees.
- The training covers capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization.
- `dolly-v2-12b` is not a state-of-the-art model.
- It exhibits surprisingly high-quality instruction-following behavior, not characteristic of the foundation model on which it is based.
- Smaller model sizes of Dolly v2 are available:
  - [dolly-v2-7b](https://huggingface.co/databricks/dolly-v2-7b) is a 6.9 billion parameter model based on `pythia-6.9b`.
  - [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b) is a 2.8 billion parameter model based on `pythia-2.8b`.
- For tips on running inference for various GPU configurations, refer to the [dolly GitHub repo](https://github.com/databrickslabs/dolly#getting-started-with-response-generation).
- **Owner**: Databricks, Inc.

## Model Overview
- `dolly-v2-12b` is a 12 billion parameter causal language model.
- It is created by [Databricks](https://databricks.com/).
- The model is derived from [EleutherAI's](https://www.eleuther.ai/) [Pythia-12b](https://huggingface.co/EleutherAI/pythia-12b).
- It is fine-tuned on a [~15K record instruction corpus](https://github.com/databrickslabs/dolly/tree/master/data).
- The instruction corpus was generated by Databricks employees.
- The model is released under a permissive license (CC-BY-SA).

## Usage
- To use the model with the `transformers` library on a machine with GPUs, ensure the `transformers` and `accelerate` libraries are installed.
- In a Databricks notebook, you could run:
  ```python
  %pip install "accelerate>=0.16.0,<1" "transformers[torch]>=4.28.1,<5" "torch>=1.13.1,<2"
  ```
- The instruction-following pipeline can be loaded using the `pipeline` function.
- The custom `InstructionTextGenerationPipeline` can be found in the model repo [here](https://huggingface.co/databricks/dolly-v2-3b/blob/main/instruct_pipeline.py).
- `trust_remote_code=True` is required to load the pipeline.
- Including `torch_dtype=torch.bfloat16` is generally recommended to reduce memory usage.
- It does not appear to impact output quality.
- It is fine to remove `torch_dtype` if there is sufficient memory.
- Code snippet to load the pipeline:
  ```python
  import torch
  from transformers import pipeline

  generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map="auto")
  ```
- You can use the pipeline to answer instructions:
  ```python
  res = generate_text("Explain to me the difference between nuclear fission and fusion.")
  print(res[0]["generated_text"])
  ```
- Alternatively, to avoid using `trust_remote_code=True`, download [instruct_pipeline.py](https://huggingface.co/databricks/dolly-v2-3b/blob/main/instruct_pipeline.py), store it alongside your notebook, and construct the pipeline yourself:
  ```python
  import torch
  from instruct_pipeline import InstructionTextGenerationPipeline
  from transformers import AutoModelForCausalLM, AutoTokenizer

  tokenizer = AutoTokenizer.from_pretrained("databricks/dolly-v2-12b", padding_side="left")
  model = AutoModelForCausalLM.from_pretrained("databricks/dolly-v2-12b", device_map="auto", torch_dtype=torch.bfloat16)

  generate_text = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)
  ```

### LangChain Usage
- To use the pipeline with LangChain, set `return_full_text=True`, as LangChain expects the full text to be returned 
and the default for the pipeline is to only return the new text.
- Code snippet for LangChain usage:
  ```python
  import torch
  from transformers import pipeline

  generate_text = pipeline(model="databricks/dolly-v2-12b", torch_dtype=torch.bfloat16,
                           trust_remote_code=True, device_map="auto", return_full_text=True)
  ```
- You can create a prompt that has only an instruction or an instruction with context:
  ```python
  from langchain import PromptTemplate, LLMChain
  from langchain.llms import HuggingFacePipeline

  # template for an instruction with no input
  prompt = PromptTemplate(
      input_variables=["instruction"],
      template="{instruction}")

  # template for an instruction with input
  prompt_with_context = PromptTemplate(
      input_variables=["instruction", "context"],
      template="{instruction}\n\nInput:\n{context}")

  hf_pipeline = HuggingFacePipeline(pipeline=generate_text)

  llm_chain = LLMChain(llm=hf_pipeline, prompt=prompt)
  llm_context_chain = LLMChain(llm=hf_pipeline, prompt=prompt_with_context)
  ```
- Example predicting using a simple instruction:
  ```python
  print(llm_chain.predict(instruction="Explain to me the difference between nuclear fission and fusion.").lstrip())
  ```
- Example predicting using an instruction with context:
  ```python
  context = """George Washington (February 22, 1732[b] - December 14, 1799) was an American military officer, statesman,
  and Founding Father who served as the first president of the United States from 1789 to 1797."""

  print(llm_context_chain.predict(instruction="When was George Washington president?", context=context).lstrip())
  ```

## Known Limitations

### Performance Limitations
- **`dolly-v2-12b` is not a state-of-the-art generative language model.**
- Quantitative benchmarking is ongoing.
- The model is not designed to perform competitively with more modern model architectures or models with larger pretraining corpuses.
- The Dolly model family is under active development.
- Known limitations include struggles with:
  - Syntactically complex prompts.
  - Programming problems.
  - Mathematical operations.
  - Factual errors.
  - Dates and times.
  - Open-ended question answering.
  - Hallucination.
  - Enumerating lists of specific length.
  - Stylistic mimicry.
  - Having a sense of humor.
- `dolly-v2-12b` lacks some capabilities present in the original model, such as well-formatted letter writing.

### Dataset Limitations
- `dolly-v2-12b` reflects the content and limitations of its training corpuses.
- **The Pile**: GPT-J's pre-training corpus contains content mostly collected from the public internet.
- The model may reflect shortcomings of web-scale datasets, including objectionable content.
- As such, the model is likely to reflect these shortcomings, potentially overtly in the case it is explicitly asked to produce objectionable content, and sometimes subtly, as in the case of biased or harmful implicit associations.
- **`databricks-dolly-15k`**: The training data represents natural language instructions generated by Databricks employees during March and April 2023.
- The dataset includes passages from Wikipedia as reference passages for instruction categories like closed QA and summarization.
- It does not contain obscenity, intellectual property, or personally identifying information about non-public figures.
- The dataset may contain typos and factual errors.
- The dataset may reflect biases found in Wikipedia.
- The dataset likely reflects the interests and semantic choices of Databricks employees, which is not representative of the global population.

- Databricks is committed to ongoing research and development efforts to develop helpful, honest, and harmless AI technologies.

### Benchmark Metrics
- Various models benchmark performance on the [EleutherAI LLM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness).
- Model results are sorted by geometric mean for intelligible ordering.
- `dolly-v2-12b` is not state-of-the-art and underperforms `dolly-v1-6b` in some evaluation benchmarks.
- The performance variations may be due to the composition and size of the underlying fine-tuning datasets.
- A robust statement regarding the sources of these variations requires further study.

|  model                            |   openbookqa |   arc_easy |   winogrande |   hellaswag |   arc_challenge |     piqa |    boolq |    gmean |
| --------------------------------- | ------------ | ---------- | ------------ | ----------- | --------------- | -------- | -------- | ---------|
| EleutherAI/pythia-2.8b            |        0.348 |   0.585859 |     0.589582 |    0.591217 |        0.323379 | 0.73395  | 0.638226 | 0.523431 |
| EleutherAI/pythia-6.9b            |        0.368 |   0.604798 |     0.608524 |    0.631548 |        0.343857 | 0.761153 | 0.6263   | 0.543567 |
| databricks/dolly-v2-3b            |        0.384 |   0.611532 |     0.589582 |    0.650767 |        0.370307 | 0.742655 | 0.575535 | 0.544886 |
| EleutherAI/pythia-12b             |        0.364 |   0.627104 |     0.636148 |    0.668094 |        0.346416 | 0.760065 | 0.673394 | 0.559676 |
| EleutherAI/gpt-j-6B               |        0.382 |   0.621633 |     0.651144 |    0.662617 |        0.363481 | 0.761153 | 0.655963 | 0.565936 |
| databricks/dolly-v2-12b           |        0.408 |   0.63931  |     0.616417 |    0.707927 |        0.388225 | 0.757889 | 0.568196 | 0.56781  |
| databricks/dolly-v2-7b            |        0.392 |   0.633838 |     0.607735 |    0.686517 |        0.406997 | 0.750816 | 0.644037 | 0.573487 |
| databricks/dolly-v1-6b            |        0.41  |   0.62963  |     0.643252 |    0.676758 |        0.384812 | 0.773667 | 0.687768 | 0.583431 |
| EleutherAI/gpt-neox-20b           |        0.402 |   0.683923 |     0.656669 |    0.7142   |        0.408703 | 0.784004 | 0.695413 | 0.602236 |

## Citation
```
@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}
```
- **Happy Hacking!**
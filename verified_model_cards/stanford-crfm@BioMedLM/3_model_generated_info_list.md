# Model Card for BioMedLM 2.7B
- Note: This model was previously known as PubMedGPT 2.7B.
- The name change was due to a request from the NIH, which holds the trademark for "PubMed".
- Paper: [BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text](https://arxiv.org/abs/2403.18421).
- BioMedLM 2.7B is a new language model.
- The model was trained exclusively on biomedical abstracts and papers from [The Pile](https://pile.eleuther.ai/).
- This is a GPT-style model.
- The model can achieve strong results on a variety of biomedical NLP tasks.
- It has a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.
- BioMedLM 2.7B is an autoregressive language model.
- The model is capable of natural language generation.
- The exploration of the generation capabilities and limitations of this model has only just begun.
- The generation capabilities of this model are for research purposes only.
- The model is not suitable for production use.
- The release of this model aims to advance the development of biomedical NLP applications.
- The release also aims to promote best practices for responsibly training and utilizing domain-specific language models.
- Issues of reliability, truthfulness, and explainability are important considerations.
- This model was a joint collaboration of [Stanford CRFM](https://crfm.stanford.edu/) and [MosaicML](https://www.mosaicml.com/).

# Table of Contents
- [Model Card for BioMedLM 2.7B](#model-card-for--model_id-)
- [Table of Contents](#table-of-contents)
- [Model Details](#model-details)
  - [Model Description](#model-description)
- [Uses](#uses)
  - [Downstream Use](#downstream-use)
  - [Out-of-Scope Use](#out-of-scope-use)
- [Bias, Risks, and Limitations](#bias-risks-and-limitations)
  - [Recommendations](#recommendations)
- [Training Details](#training-details)
  - [Training Data](#training-data)
  - [Training Procedure](#training-procedure)
    - [Preprocessing](#preprocessing)
- [Environmental Impact](#environmental-impact)
- [Technical Specifications](#technical-specifications)
  - [Model Architecture and Objective](#model-architecture-and-objective)
  - [Compute Infrastructure](#compute-infrastructure)

# Model Details

## Model Description
- BioMedLM 2.7B is a new language model.
- The model was trained exclusively on biomedical abstracts and papers from [The Pile](https://pile.eleuther.ai/).
- This GPT-style model can achieve strong results on a variety of biomedical NLP tasks.
- It has a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.
- BioMedLM 2.7B is an autoregressive language model.
- The model is capable of natural language generation.
- The exploration of the generation capabilities and limitations of this model has only just begun.
- The generation capabilities of this model are for research purposes only.
- The model is not suitable for production use.
- The release of this model aims to advance the development of biomedical NLP applications.
- The release also aims to promote best practices for responsibly training and utilizing domain-specific language models.
- Issues of reliability, truthfulness, and explainability are important considerations.
- This model was a joint collaboration of [Stanford CRFM](https://crfm.stanford.edu/) and [MosaicML](https://www.mosaicml.com/).
- **Developed by:** Stanford CRFM, MosaicML
- **Shared by:** Stanford CRFM
- **Model type:** Language model
- **Language(s) (NLP):** en
- **License:** [bigscience-bloom-rail-1.0](https://huggingface.co/spaces/bigscience/license)

# Uses
- This model is licensed under the terms of [BigScience Open RAIL-M license](https://huggingface.co/spaces/bigscience/license).
- The license is used for [BLOOM](https://huggingface.co/bigscience/bloom-1b1).
- The license forbids use of the model (or derivatives thereof) "To provide medical advice and medical results interpretation."
- If there are concerns about a use case falling under the restriction, it is possible to contact for discussion.

## Direct Use
- It is possible to use this model to generate text.
- Generating text is useful for experimentation and understanding the model's capabilities.
- The model should not be directly used for production or work that may directly impact people.

## Downstream Use
- The main way this model has been used is fine-tuning for downstream question answering tasks.
- It is recommended to use this model in that way.

## Out-of-Scope Use
- The model is not recommended for natural language generation in a production environment, whether fine-tuned or otherwise.

# Bias, Risks, and Limitations
- Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf)).
- Predictions generated by the model may include disturbing and harmful stereotypes across protected classes.
- Predictions may include stereotypes related to identity characteristics and sensitive, social, and occupational groups.

## Recommendations
- While the model is capable of generating natural language text, exploration of this capability and its limitations has only just begun.
- Understanding these limitations is especially important in a domain like medicine.
- Therefore, **we strongly recommend against using this model in production for natural language generation.**

# Training Details

## Training Data
- This model was trained on the Pubmed Abstracts and Full Text from [The Pile](https://pile.eleuther.ai/).

## Training Procedure
- The model was trained on [MosaicML Cloud](https://www.mosaicml.com/cloud).
- MosaicML Cloud is a platform designed for large workloads like LLMs.
- The training used the [Composer](https://github.com/mosaicml/composer) training library.
- The training also used [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html).
- Multi-node training was enabled across 128 A100-40GB GPUs.
- The total run was completed in approximately 6.25 days.
- The model was trained with a batch size of 1024 and a sequence length of 1024 for 300B tokens.
- The training used Decoupled AdamW with the following settings:
  |  |  |
  | --- | ------ |
  | lr  | 1.6e-4 |
  | eps | 1e-8       |
  | betas | \[0.9, 0.95\]      |
  | weight decay    |  1.6e-5      |
- The training process was very smooth and did not suffer from any divergences.
- There was uncertainty about the benefits of training out to 300B tokens for language model perplexity and downstream task performance.
- Most models of this scale (e.g., GPT Neo 2.7B) are trained to 300-400B tokens.
- The datasets those models use are vastly larger than PubMed.
- The Pile is 8x the size of its PubMed subcorpora.
- Steady perplexity improvements were observed on the validation and training sets throughout the training.
- Preliminary experiments showed improved downstream task performance as training continued to the full 300B tokens.
- The conclusion was that it was worth it to train for the full 300B tokens, despite more passes through the data than comparable models.

### Preprocessing
- The model uses a custom tokenizer trained on the PubMed Abstracts.
- It is important to use a tokenizer trained on in-domain text to maximize performance on downstream tasks.
- A key benefit is that common biomedical terms are represented as entire tokens.
- The following terms are tokenized into single tokens by the biomedical tokenizer:
  |   |   |
  | --- | --- |
  | chromatography |   chrom/atography |
  | cytotoxicity |   cyt/ot/oxicity |
  | Immunohistochemistry |   Immun/oh/ist/ochemistry |
  | photosynthesis |   photos/ynthesis |
  | probiotic |   prob/iotic |
- This allows the model to encode information about these concepts in their individual token representations rather than spread out across subword tokens.

# Technical Specifications

## Model Architecture and Objective
- BioMedLM 2.7B is a standard GPT-2 implementation (trained with Flash Attention).
- The model has the following hyperparameters:
  |             |       |
  | ----------- | ----- |
  | hidden size | 2560  |
  | heads       | 20    |
  | layers      | 32    |
  | vocab size  | 28896 |
  | sequence length| 1024      |

## Compute Infrastructure
- The model was trained on [MosaicML Cloud](https://www.mosaicml.com/cloud).
- MosaicML Cloud is a platform designed for large workloads like LLMs.
- The training used the [Composer](https://github.com/mosaicml/composer) training library.
- The training also used [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html).
- Multi-node training was enabled across 128 A100-40GB GPUs.
- The total run was completed in approximately 6.25 days.